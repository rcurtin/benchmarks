'''
  @file perceptron.py
  @author Anand Soni

  Perceptron classification with scikit.
'''

import os
import sys
import inspect

# Import the util path, this method even works if the path contains symlinks to
# modules.
cmd_subfolder = os.path.realpath(os.path.abspath(os.path.join(
  os.path.split(inspect.getfile(inspect.currentframe()))[0], "../../util")))
if cmd_subfolder not in sys.path:
  sys.path.insert(0, cmd_subfolder)

#Import the metrics definitions path.
metrics_folder = os.path.realpath(os.path.abspath(os.path.join(
  os.path.split(inspect.getfile(inspect.currentframe()))[0], "../metrics")))
if metrics_folder not in sys.path:
  sys.path.insert(0, metrics_folder)

from log import *
from timer import *
from definitions import *
from misc import *

import numpy as np
from sklearn.linear_model import Perceptron

'''
This class implements the Perceptron benchmark.
'''
class PERCEPTRON(object):

  '''
  Create the Perceptron benchmark instance.

  @param dataset - Input dataset to perform Perceptron classification on.
  @param timeout - The time until the timeout. Default no timeout.
  @param verbose - Display informational messages.
  '''
  def __init__(self, dataset, timeout=0, verbose=True):
    self.verbose = verbose
    self.dataset = dataset
    self.timeout = timeout
    self.model = None
    self.predictions = None
    self.opts = {}

  '''
  Build the model for the Perceptron.

  @param data - The train data.
  @param responses - The responses for the train set.
  @return The created model.
  '''
  def BuildModel(self, data, responses):
    # Create and train the classifier.
    p = Perceptron(**opts)
    p.fit(data, responses)
    return p

  '''
  Use the scikit libary to implement Perceptron.

  @param options - Extra options for the method.
  @return - Elapsed time in seconds or a negative value if the method was not
  successful.
  '''
  def PerceptronScikit(self, options):
    def RunPerceptronScikit(q):
      totalTimer = Timer()

      # Load input dataset.
      # If the dataset contains two files then the second file is the test file.
      Log.Info("Loading dataset", self.verbose)
      if len(self.dataset) >= 2:
        testSet = LoadDataset(self.dataset[1])
      else:
        Log.Fatal("This method requires atleast two datasets.")

      # Gather all parameters.
      self.opts = {}
      if "max_iterations" in options:
        self.opts["n_iter"] = int(options.pop("max_iterations"))

      if len(options) > 0:
        Log.Fatal("Unknown parameters: " + str(options))
        raise Exception("unknown parameters")

      # Use the last row of the training set as the responses.
      X, y = SplitTrainData(self.dataset)

      try:
        with totalTimer:
          # Perform perceptron classification.
          self.model = self.BuildModel(X, y)
          if len(self.dataset) >= 2:
            self.predictions = self.model.predict(testSet)
      except Exception as e:
        q.put([-1])
        return -1

      time = totalTimer.ElapsedTime()
      if len(self.dataset) > 1:
        q.put([time, self.predictions])
      else:
        q.put([time])
        
      return time

    result = timeout(RunPerceptronScikit, self.timeout)
    if len(result) > 1:
      self.predictions = result[1]
    
    return result[0]

  '''
  Perform Perceptron Classification. If the method has been successfully completed
  return the elapsed time in seconds.

  @param options - Extra options for the method.
  @return - Elapsed time in seconds or a negative value if the method was not
  successful.
  '''
  def RunMetrics(self, options):
    Log.Info("Perform Perceptron Classification.", self.verbose)

    results = self.PerceptronScikit(options)
    if results < 0:
      return results

    metrics = {'Runtime' : results}

    if len(self.dataset) >= 3:

      truelabels = LoadDataset(self.dataset[2])

      confusionMatrix = Metrics.ConfusionMatrix(truelabels, self.predictions)
      metrics['ACC'] = Metrics.AverageAccuracy(confusionMatrix)
      metrics['LFT'] = Metrics.LiftMultiClass(confusionMatrix)
      metrics['MCC'] = Metrics.MCCMultiClass(confusionMatrix)
      # metrics['FMeasure'] = Metrics.AvgFMeasure(confusionMatrix)
      metrics['Precision'] = Metrics.AvgPrecision(confusionMatrix)
      metrics['Recall'] = Metrics.AvgRecall(confusionMatrix)
      metrics['MSE'] = Metrics.SimpleMeanSquaredError(truelabels, self.predictions)

    return metrics
